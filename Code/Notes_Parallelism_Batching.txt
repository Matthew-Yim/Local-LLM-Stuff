Batching:
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# Example queries
queries = ["Hello, how are you?", "What's the weather like today?", "Tell me a joke."]

# Tokenize and batch the queries
inputs = tokenizer(queries, return_tensors="pt", padding=True, truncation=True)

# Generate responses
outputs = model.generate(**inputs)

# Decode the responses
responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)

for query, response in zip(queries, responses):
    print(f"Query: {query}\nResponse: {response}\n")


Parallel Processing:
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# Check if multiple GPUs are available and wrap the model
if torch.cuda.device_count() > 1:
    print("Using", torch.cuda.device_count(), "GPUs")
    model = torch.nn.DataParallel(model)

model.to("cuda")

# Example queries
queries = ["Hello, how are you?", "What's the weather like today?", "Tell me a joke."]

# Tokenize and batch the queries
inputs = tokenizer(queries, return_tensors="pt", padding=True, truncation=True)

# Generate responses
outputs = model.generate(**inputs)

# Decode the responses
responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)

for query, response in zip(queries, responses):
    print(f"Query: {query}\nResponse: {response}\n")


Multiple GPU's to load up LLM not to make inferences
from transformers import pipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
import time

import torch
from accelerate import init_empty_weights, load_checkpoint_and_dispatch

t1= time.perf_counter()
tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLlama-34b-Instruct-hf")
model = AutoModelForCausalLM.from_pretrained("codellama/CodeLlama-34b-Instruct-hf", device_map="auto")

t2= time.perf_counter()
print(f"Loading tokenizer and model: took {t2-t1} seconds to execute.")
# Create a pipeline
code_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)

t3= time.perf_counter()
print(f"Creating piepline: took {t3-t2} seconds to execute.")
# Generate code for an input string
while True:
  print("\n=========Please type in your question=========================\n")
  user_content = input("\nQuestion: ") # User question
  user_content.strip()
  t1= time.perf_counter()
  generated_code = code_generator(user_content, max_length=256)[0]['generated_text']
  t2= time.perf_counter()
  print(f"Inferencing using the model: took {t2-t1} seconds to execute.")
  print(generated_code)